import os
import uuid
import logging
from typing import List, Dict, Optional, AsyncGenerator

# å¯¼å…¥ä½ æ‹†åˆ†å‡ºæ¥çš„åŸå­åŒ–æœåŠ¡
from .vector_service import VectorService
from .document_service import DocumentService
from .llm_service import LLMService

# é…ç½®æ—¥å¿—ï¼Œè¿™æ˜¯ä¼ä¸šçº§å¼€å‘çš„æ ‡é…
logger = logging.getLogger(__name__)

class RAGFacade:
    def __init__(self):
        """åˆå§‹åŒ–ä¸‰ä½ä¸€ä½“çš„æ ¸å¿ƒæœåŠ¡"""
        self.vector_service = VectorService()
        self.doc_service = DocumentService()
        self.llm_service = LLMService()

    async def ingest_document(self, file_path: str) -> str:
        """
        ä¸‰ä½ä¸€ä½“ï¼šè§£æ -> æ¸…æ´— -> å­˜å‚¨ (å®Œå…¨å¯¹æ ‡ services.py é€»è¾‘)
        """
        try:
            file_name = os.path.basename(file_path)
            
            # 1. è§£æä¸æ¸…æ´— (è°ƒç”¨ DocumentService)
            # åŸé€»è¾‘ï¼špymupdf æå–å¹¶ clean
            full_text = self.doc_service.parse_pdf(file_path)
            
            # 2. æ–‡æœ¬åˆ‡ç‰‡
            # åŸé€»è¾‘ï¼š500 å­—ç¬¦ + 50 é‡å 
            chunks = self.doc_service.split_text(full_text)
            
            # 3. å»é‡ï¼šæŒ‰æ–‡ä»¶åæ¸…ç†æ—§æ•°æ® (è°ƒç”¨ VectorService)
            # åŸé€»è¾‘ï¼šcollection.get(where={"source": file_name}) å delete
            self.vector_service.delete_by_source(file_name)
            
            # 4. ç”Ÿæˆ ID ä¸å…ƒæ•°æ®å¹¶å­˜å…¥æ•°æ®åº“
            file_uuid = uuid.uuid4().hex[:6]
            ids = [f"{file_name}_{file_uuid}_{i}" for i in range(len(chunks))]
            metadatas = [{"source": file_name, "file_id": file_uuid} for _ in range(len(chunks))]
            
            self.vector_service.add_documents(ids, chunks, metadatas)
            
            logger.info(f"æˆåŠŸè§£æå¹¶å…¥åº“: {file_name}, å…± {len(chunks)} å—")
            return f"æˆåŠŸè§£ææ–‡ä»¶ {file_name}ï¼Œåˆ‡åˆ†ä¸º {len(chunks)} å—"
            
        except Exception as e:
            logger.error(f"è§£ææ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
            raise Exception(f"æ–‡æ¡£è§£æå…¥åº“å¤±è´¥: {str(e)}")

    async def ask_question_stream(self, user_input: str, history: List[Dict], doc_id: str = None) -> AsyncGenerator[str, None]:
        """
        æµå¼ RAG é“¾è·¯ï¼šæ£€ç´¢ -> æ‹¼ Prompt -> å¼‚æ­¥äº§ç”Ÿ Token
        """
        try:
            # 2. æ£€ç´¢é€»è¾‘ä¿æŒä¸å˜ï¼ˆå¿…é¡»å…ˆæ‹¿åˆ°ä¸Šä¸‹æ–‡æ‰èƒ½å¼€å§‹è¯´è¯ï¼‰
            search_results = self.vector_service.query(
                user_input, 
                n_results=3,
                doc_id=doc_id
            )
            
            # 2. é˜ˆå€¼è¿‡æ»¤ä¸ä¸Šä¸‹æ–‡æ„å»º
            # åŸé€»è¾‘ï¼šthreshold = 0.8
            threshold = 0.8
            filtered_contexts = []
            
            documents = search_results["documents"][0]
            distances = search_results["distances"][0]
            metadatas = search_results["metadatas"][0]
            
            for doc, dist, meta in zip(documents, distances, metadatas):
                if dist < threshold:
                    source = meta.get("source", "æœªçŸ¥æ–‡æ¡£")
                    filtered_contexts.append(f"ã€æ¥æº: {source}ã€‘: {doc}")
            
            # 3. æ„å»ºåŠ¨æ€ Prompt
            context_text = "\n".join(filtered_contexts)
            print(filtered_contexts)
            if not filtered_contexts:
                # å¯¹åº” services.py ä¸­çš„â€œèµ„æ–™ä¸è¶³â€å¤„ç†é€»è¾‘
                prompt = f"ç”±äºçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œä½ å¯ä»¥æ ¹æ®ä½ çš„é€šç”¨çŸ¥è¯†å›ç­”ï¼Œä½†è¦å£°æ˜è¿™ä¸æ˜¯å®˜æ–¹ç­”æ¡ˆï¼š{user_input}"
            else:
                # å¯¹åº” services.py ä¸­çš„â€œçº¦æŸè¦æ±‚â€Prompt
                prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€ä¸¥è°¨çš„ AI åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹æä¾›çš„å‚è€ƒèµ„æ–™å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
                
                ### çº¦æŸè¦æ±‚:
                1. å¿…é¡»ä¼˜å…ˆä½¿ç”¨å‚è€ƒèµ„æ–™ä¸­çš„ä¿¡æ¯ã€‚
                2. å¦‚æœèµ„æ–™ä¸­æ²¡æœ‰æåˆ°ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯šå®å‘ŠçŸ¥ã€‚
                3. å›ç­”ç»“æŸåï¼Œè¯·åŠ¡å¿…æ³¨æ˜ä½ å‚è€ƒçš„ã€æ¥æºã€‘æ–‡ä»¶åã€‚
                
                ### å‚è€ƒèµ„æ–™:
                {context_text}
                
                ### ç”¨æˆ·é—®é¢˜:
                {user_input}
                """

            # 4. æ„å»ºæ¶ˆæ¯å†å² (æ‹’ç»å…¨å±€å˜é‡ï¼Œä½¿ç”¨ä¼ å…¥çš„ history)
            # æ ¸å¿ƒä¿®æ­£ï¼šè¿™é‡Œæˆ‘ä»¬åªæ„å»ºæœ¬æ¬¡è¯·æ±‚çš„ messagesï¼Œä¸ä¿®æ”¹å¤–éƒ¨ history çŠ¶æ€
            messages_to_send = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€é£è¶£çš„ AI åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä½ è·å¾—çš„çŸ¥è¯†å†…å®¹è¿›è¡Œå›ç­”ã€‚"}
            ] + history + [{"role": "user", "content": prompt}]

            # 5. ğŸ† æ ¸å¿ƒé‡æ„ï¼šè°ƒç”¨ LLM çš„æµå¼æ–¹æ³•å¹¶ yield Token
            # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ä½ çš„ llm_service æœ‰ä¸€ä¸ªåä¸º generate_response_stream çš„ç”Ÿæˆå™¨æ–¹æ³•
            async for chunk in self.llm_service.generate_response_stream(messages_to_send):
                if chunk:
                    yield chunk  # å®æ—¶å°†æ¯ä¸€ä¸ªå­—ï¼ˆTokenï¼‰å‘é€ç»™ä¸Šå±‚æ¥å£

        except Exception as e:
            logger.error(f"æµå¼é—®ç­”æµç¨‹å¤±è´¥: {e}")
            yield "æŠ±æ­‰ï¼Œæˆ‘çš„å¤§è„‘ä¿¡å·æœ‰ç‚¹ä¸ç¨³å®šï¼Œè¯·å°è¯•é‡æ–°å‘é€ã€‚"
        
    # app/services/rag_facade.py å¢åŠ ä»¥ä¸‹æ–¹æ³•
    async def get_document_list(self) -> list:
        """è·å–æ‰€æœ‰å·²å…¥åº“çš„æ–‡æ¡£åˆ—è¡¨"""
        try:
            return self.vector_service.get_unique_sources()
        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}")
            return []